#------------------------------------------------------------------#
# must check the direction of data_root and result_root before run #
#------------------------------------------------------------------#

#configure feature path
# **** please must modify the data_root before first running ****
data_root: &d_r ./data
modelnet40_ft: !join [*d_r, ModelNet40_mvcnn_gvcnn.mat]
ntu2012_ft: !join [*d_r, NTU2012_mvcnn_gvcnn.mat]

# Converted hypergraph datasets - EXPLICIT versions (uses original hypergraph structure)
coauthorship_cora_ft: !join [*d_r, converted_hgnn/coauthorship_cora_explicit.mat]
coauthorship_dblp_ft: !join [*d_r, converted_hgnn/coauthorship_dblp_explicit.mat]
cocitation_cora_ft: !join [*d_r, converted_hgnn/cocitation_cora_explicit.mat]
cocitation_citeseer_ft: !join [*d_r, converted_hgnn/cocitation_citeseer_explicit.mat]
cocitation_pubmed_ft: !join [*d_r, converted_hgnn/cocitation_pubmed_explicit.mat]

# Add flag to use explicit hypergraphs
use_explicit_hypergraph: True  # Test explicit with balanced regularization


#Hypergraph
graph_type: &g_t hypergraph
K_neigs: [10]
#K_neigs: [10, 15 ]
m_prob: 1.0
is_probH: True
#---------------------------------------
# change me
use_mvcnn_feature_for_structure: True
use_gvcnn_feature_for_structure: True
#---------------------------------------


#Model
#--------------------------------------------------
# select the dataset you use
#on_dataset: &o_d ModelNet40
#on_dataset: &o_d NTU2012
# Your hypergraph datasets:
on_dataset: &o_d CoauthorshipCora
#on_dataset: &o_d CoauthorshipDblp  
#on_dataset: &o_d CocitationCora
#on_dataset: &o_d CocitationCiteseer
#on_dataset: &o_d CocitationPubmed
#--------------------------------------------------

#---------------------------------------
# change me
use_mvcnn_feature: False
use_gvcnn_feature: True
#---------------------------------------


#Result
# configure result path
# **** please must modify the result_root before first running ****
result_root: &r_r ./results
result_sub_folder: !join [*r_r, !concat [ *g_t, _, *o_d ]]
ckpt_folder: !join [*r_r, ckpt]


#Train
max_epoch: 600
n_hid: 128
lr: 0.0005  # Stable learning rate  
milestones: [100]
gamma: 0.9
drop_out: 0.7  # Strong dropout for regularization
print_freq: 50
weight_decay: 0.005  # Strong weight decay
decay_step: 200
decay_rate: 0.7
# Early stopping
patience: 50  # Balanced early stopping